# парсер сайта на ASPX
Это было тестовое задания компании. Суть задания: Сделать парсинг https://catalogue.ite-expo.ru/ru-RU/exhibitorlist.aspx?project_id=507 (если получится).
Подобного гита я не видел нигде, так что решил залить свой


## Технологии
-Node.JS 
-Puppeteer


## Использование
Установить Node.JS, подключить Puppeteer.
В самом скрипте (index.js) изменить константы:
    "linkStr1" и "linkStr2" - части ссылки для переход на новую страницы, между ними, должен стоять номер нужной страницы
    "countPages" - количество страниц для парсинга. (Да, можно было добавить автоматическую проверку на последнюю страницы, но в задании это не указано)
    "selectorForFind" - уникальное название селектора, он нужен для проверки обновления страницы, чтобы удостовериться, что DOM подгрузился полностью и можно вытаскивать данные. В моем случае это был селектор "strong", но в вашем случае нужно смотреть самостоятельно.
    "url" - адрес сайта для парсинга

### Нюансы
Понятное дело, код может/должен быть изменен под конкретный сайт. Но как пример, мне кажется, этот код отлично подходит.

### В чем была проблема?
Проблема была в том, что это не простой сайт с подгрузкой новых строк или сайт с постраничным выводом, где номер страницы содержится в адресной строке. Это сайт, где адресная строка не меняется, вывод новых строк происходит в форму, а переход между страницами идет через __doPostBack(). 
Изначально я делал это на php, но наткнушись на эту проблему, я решил сменить технологии (спустя N часов поиска информации). 
В итоге, выбор пал на Node.JS, так как проблему на Js поможет решить только Js =)

### Суть работы скрипта
Суть проста:
1) открываем браузер
2) переходим на страницу
3) циклом проходимся по всем страницам
4) собираем инфу со всех страниц
5) выгружаем в JSON


## FAQ 
Пока нет



